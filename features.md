---
layout: page
title: Projects
permalink: /projectsDepricated/
---

### [Decomposing Fit Semantics for Product Size Recommendation in Metric Spaces](https://github.com/rishabhmisra/Product-Catalog-Size-Recommendation-Framework)
Product size recommendation and fit prediction are critical in order to improve customers’ shopping experiences and to reduce product return rates. Modeling customers’ fit feedback is challenging due to its subtle semantics, arising from the subjective evaluation of products, and imbalanced label distribution. In this paper, we propose a new predictive framework to tackle the product fit problem, which captures the semantics behind customers’ fit feedback, and employs a metric learning technique to resolve label imbalance issues. We also contribute two public datasets collected from online clothing retailers.

### [Sarcasm Detection using Hybrid Neural Network](https://github.com/rishabhmisra/Sarcasm-Detection-using-NN)
Sarcasm Detection has enjoyed great interest from the research community, however the task of predicting sarcasm in a text remains an elusive problem for machines. Past studies mostly make use of twitter datasets collected using hashtag based supervision but such datasets are noisy in terms of labels and language. To overcome these shortcoming, we introduce a new dataset which contains news headlines from a sarcastic news website and a real news website. Next, we propose a hybrid Neural Network architecture with attention mechanism which provides insights about what actually makes sentences sarcastic. Through experiments, we show that the proposed model improves upon the baseline by ∼ 5% in terms of classification accuracy.

### [Jointly Modeling Aspects, Ratings and Sentiments With Temporal Dynamics](https://github.com/rishabhmisra/t-jmars)
In this work, we first implement and then propose an extension to the paper Jointly Modeling Aspects, Ratings and Sentiments for movie recommendation (JMARS) by carefully incorporating temporal dynamics into the model and empirically showing that it provides an improvement of around 0.9% on two datasets. We replicate the quantitative results and qualitative analysis provided in the original paper and furthermore, provide some analysis from the extended model.

### [Hierarchical Attention Network for Rating Prediction](https://github.com/rishabhmisra/Hierarchical-Attention-Network)
Implementation of an RNN with attention mechanism for rating prediction using product reviews. Attention mechanism allows the RNN to focus on sentences which best explain the rating given to an item.

### [Hotel Recommendation System](https://github.com/rishabhmisra/Hotel-Recommendation-System)
In this project, we explore Expedia's online hotel booking dataset to recommend hotels to users based on their preferences. The dataset was made available by Expedia as a Kaggle challenge. In this project, our aim is to contextualize customer data and predict the likelihood a user will stay at 100 different hotel groups. The goal is to provide not just one recommendation but to rank the predictions and return the top five most likely hotel clusters for each user’s particular search query in the test set. We use multiple models and techniques to arrive at our best solution. This includes (1) An ensemble of four different models (random forests, SGD classifier, XG Boost and Naive Bayes), (2) XG Boost that is preceded by completion of distance (between user and their potential destination) matrix - an important feature in the dataset which is currently incomplete, (3) Data leakage solution which takes advantage of the fact that there is a potential leak in the data provided to us, and (4) a mixture of the methods in (1) and (3).

### [VARACNet: An Ensemble of Convolutional Neural Networks for Traffic Lights Recognition](https://github.com/rishabhmisra/VARACNet/blob/master/VARACNet.pdf)
We developed VARACNet, an ensemble of multiple Convolutional Neural Networks (CNN) where each CNN is built with the motive of giving a superior performance while keeping the model size small. The sub-models have no more than 490k parameters but each achieves an accuracy greater than 87%. Models are tested and trained on the Nexar traffic lights challenge dataset with the aim of correctly recognizing the presence and state of traffic lights in images taken by the drivers using the Nexar app. We show that minimizing the number of parameters in each of the models allows quick training even when computational resources are not abundant. We also perform localization experiments using Faster R-CNN on a separate annotated dataset (UCSD traffic lights dataset) to demonstrate high performance for classification and detection tasks. The output is highly desirable and useful for deployment onto actual vehicles where the memory is limited and traffic light inferences have to be made in real-time.

### [Music Generation using Character-level RNN](https://github.com/rishabhmisra/Music-Generation-using-RNN)
In this project, we train a Recurrent Neural Network (RNN) to learn the structure of music files in
ABC format and then generate a music file based on what the model learns. We built a simple network with 1 hidden layer and fed the input sequence of length 25 to predict the next character in sequence. We experiment with different parameters like temperature, dropout, the number of neurons in the hidden layer and the effect of optimizers and observe their effect on model’s learning behavior. We also visualize the activations of neurons in RNN to understand the purpose of each hidden unit in the network.

### [Prediction of Helpfulness of Reviews](https://github.com/rishabhmisra/Helpfulness-Prediction)
This project was done as a part of a Kaggle competition (inclass.kaggle.com/c/cse158-258-helpfulness-prediction). The dataset was Amazon product reviews dataset with 200,000 training examples and 14,000 test examples. Our aim was to predict the number of helpful votes a review would get out of all the votes given to the review. Accuracy was measured in terms of the mean absolute error (MAE). After pre-processing and cleaning of the data, I incorporated features which were the polynomial interactions (of degree 2) of some of the best attributes in the data and consequently applied linear regression. Please follow the link to the project to see my report where I've described my approach and model in detail. I ended up in top 12% at the end of the competition.

### [Product Rating Prediction](https://github.com/rishabhmisra/Rating-Prediction)
This project was done as a part of a Kaggle competition (inclass.kaggle.com/c/cse258-rating-prediction). The dataset was Amazon product reviews dataset with 200,000 training examples and 14,000 test examples. Our aim was to predict the rating user would give to an item. Accuracy was measured in terms of the mean squared error (MAE). I employed matrix factorization method with user and item biases to predict the missing ratings. I ended up in top 17% at the end of the competition.

### [Scalable Bayesian Matrix Factorization](https://github.com/rishabhmisra/Scalable-Bayesian-Matrix-Factorization)
Factor based models have been used extensively in recommender systems based on collaborative filtering. Matrix Factorization is the simplest and most well studied factor based model and has been applied successfully in several domains. A fully Bayesian treatment of Matrix Factorization avoids the problem of overfitting and doesn't entail manual tuning of parameters, but the existing Bayesian Matrix Factorization method, based on the Markov-Chain-Monte-Carlo technique, has cubic time complexity with respect to the dimension of latent factors, which makes it difficult to apply it to very large datasets. In this project, we have proposed a scalable Bayesian Matrix Factorization approach, again based on the Markov-Chain-Monte-Carlo technique, but which has linear time complexity with respect to the dimension of latent factors and linear space complexity with respect to the number of training instances. Also, we show through extensive experiments on three sufficiently large real-world datasets that our method incurs only a small loss in the performance and takes much less time as compared to the baseline method for higher latent dimension.

### [Scalable Variational Bayesian Factorization Machine](https://github.com/rishabhmisra/Scalable-Variational-Bayesian-Factorization-Machine)
Factorization Machine (FM) is a generic and powerful framework. Though FM has Bayesian MCMC sampling based inference, for very large data MCMC is difficult to apply due to convergence issues. We have developed variational Bayesian inference for FM which is an alternative to MCMC.
